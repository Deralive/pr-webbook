\chapter{频率学派中点估计的常见评价方法}
\begin{introduction}
\item Prob $\&$ Stat\quad6.1 \quad 6.2\quad 6.4
\end{introduction}
\section{引导问题}
在前一讲中，我们介绍了多种点估计方法，自然有一个问题：怎样的估计是一个\textbf{好}的点估计？
	
一般认为一个好的点估计，应该有一些合理性的要求。于是，什么是合理性的要求？

\begin{remark}
    在本讲只讨论一维参数估计的性质或评价方法，这些性质或评价方法也可以推广到多维参数。
\end{remark}

\section{有限样本的评估方式——均方误差}

在评估估计方法时，一个非常直观的想法是比较我得到的估计值与真实值之间的差异。如果差异越小，估计方法越好。而我们所得到的估计值本身是样本的函数，不仅仅依赖于函数的构造（估计方法），也依赖于样本的质量。我们采取了“平均化”的策略尽可能地消除样本所造成的影响。于是，我们构建了以下指标——均方误差。

\begin{definition}[均方误差]
    若$\hat{\theta}$是参数$\theta$的一个估计。称
    $$
    \text{MSE}(\hat{\theta}) = E(\hat{\theta}-\theta)^2
    $$
    为$\hat{\theta}$的均方误差（Mean Squared Error, MSE）。
\end{definition}
\begin{remark}均方误差是评价点估计的最一般的标准。希望点估计的均方误差越小越好。
\end{remark}

通过分解，我们可以发现
\begin{eqnarray*}
\text{MSE}(\hat{\theta})&=& E(\hat{\theta}-\theta)^{2} \\
&=& E(\hat{\theta}-E(\hat{\theta})+E(\hat{\theta})-\theta)^{2} \\
&=& E(\hat{\theta}-E(\hat{\theta}))^{2}+2 E(\hat{\theta}-E(\hat{\theta})) \cdot(E(\hat{\theta})-\theta) +E(E(\hat{\theta})-\theta)^{2} 
\\
&=& E(\hat{\theta}-E(\hat{\theta}))^{2} +(E(\hat{\theta})-\theta)^{2} 
\end{eqnarray*}
其中，最后一个等式成立，是因为交叉项$E\left((\hat{\theta}-E(\hat{\theta})) \cdot(E(\hat{\theta})-\theta) \right)=0$。

\begin{remark}
    点估计的均方误差可以分解为两个部分：
    \begin{enumerate}
        \item $E(\hat{\theta}-E(\hat{\theta}))^{2}$可记为$\text{Var}(\hat{\theta})$，表示点估计的方差；
        \item $(E(\hat{\theta})-\theta)^{2} $可记为$\text{Bias}^2(\hat{\theta})$，表示点估计偏差的平方。
    \end{enumerate}
\end{remark}



\subsection{无偏性}

估计的无偏性是最常见的性质，它是重要的，但不是必要的。

\begin{definition}{无偏性}
	设$\theta$是我们待估计的参数，而$\hat{\theta}$是$\theta$的一个点估计。如果$\hat{\theta}$满足
	$$
	E_{\theta} (\hat{\theta}) = \theta, \theta \in \theta.
	$$
	则称$\hat{\theta}$是$\theta$的无偏估计（Unbiased  Estimate，U.E.）。
\end{definition}
\begin{remark}
    倘若一个点估计是无偏估计，这个点估计的偏差为零，即$E_{\theta} (\hat{\theta}) -\theta =0$；而与无偏估计相对，我们统一地称不具有无偏估计的估计是有偏估计。
\end{remark}

\begin{example}
	若总体分布为一个未知分布，其分布函数记为$F(x)$。设其期望为$\mu$，即$E(X) = \mu$，方差为$\sigma^2$，即$\text{Var}(X) =\sigma^2$。现有样本$x_1,x_2,\cdots,x_n$。

通常，样本均值$\bar{x}$是总体均值$\mu$的一个估计。我们可以计算$\bar{x}$的期望，即
    \begin{eqnarray*}
        E(\bar{x}) &=& E\left( \frac{1}{n} \sum_{i=1}^{n} x_{i} \right)\\
        &=& \frac{1}{n} \sum_{i=1}^{n} E\left(x_{i}\right)\\
        &=&\mu.
    \end{eqnarray*}
  于是，$\bar{x}$是$\mu$的无偏估计。

而样本方差$s^2 = \frac{1}{n} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$来估计总体方差。我们可以计算$s_n^2$的期望，即
\begin{eqnarray*}
    E(s_n^2) =  \frac{n-1}{n} \sigma^{2} \neq \sigma^2.
\end{eqnarray*}
    于是，$s_n^2$不是$\sigma^2$的无偏估计。
    
因为$E\left(s_{n}^{2}\right)=\frac{n-1}{n} \sigma^{2}$，$S_{n}^{2}$不是无偏估计。 但易于证明$s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2$是$\sigma^2$的无偏估计。
\end{example}

 \begin{remark}
 回看样本方差$s_n^2$，这个估计量的偏差为
 $$
 -\frac{1}{n}\sigma^2 .
 $$
 当样本量$n$越大时，$s_n^2$的偏差越接近0，而$s_n^2$的期望也越接近$\sigma^2$。于是，我们称$s_n^2$是$\sigma^2$的\textbf{渐近无偏}估计。
 \end{remark}

 \begin{problem}
    总体标准差$\sigma$的无偏估计会是怎样的？样本标准差$s$是$\sigma$的无偏估计吗？
\end{problem}
\begin{example}
    考虑总体分布为$N(\mu,\sigma^2)$，$\mu$和$\sigma^2$均是待估参数。现有样本$x_1,x_2,\cdots,x_n$。已知样本方差$s^2$为
    $$
    s^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i-\bar{x})^2.
    $$
    而样本标准差$s = \sqrt{s^2}$。
    为计算$s$的期望，我们先来回顾一下$s^2$的分布。我们知道在正态分布假定下，
    $$
    \frac{(n-1)s^2}{\sigma^2} \sim \chi^2(n-1) = Ga((n-1)/2,1/2)。
    $$
    因为$s = (s^2)^{1/2}$，我们来看待一般的伽马分布$Y \sim Ga(\alpha,\gamma)$的$k$阶矩的期望。
\begin{eqnarray*}
E(Y^{k}) &=& \int_{0}^{\infty} y^{k} \frac{\lambda^{\alpha}}{\Gamma(\alpha)} y^{\alpha - 1} \exp\{-\lambda y\} \text{d} y\\
     &=& \int_{0}^{\infty} \frac{\lambda^{\alpha}}{\Gamma(\alpha)} y^{k+\alpha - 1} \exp\{-\lambda y\} \text{d} y\\
     &=&\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\frac{\Gamma(\alpha + k)} {\lambda^{\alpha +k}} \int_{0}^{\infty} \frac{\lambda^{\alpha +k}}{\Gamma(\alpha + k)} y^{k+\alpha - 1} \exp\{-\lambda y\} \text{d} y\\
     &=& \frac{\Gamma(\alpha + k)}{\Gamma(\alpha)} \cdot \lambda^{-k}.
\end{eqnarray*}
于是，$$
E(s) = E\left((s^2)^{1/2}\right) = \frac{\Gamma\left(\frac{n-1}{2}+\frac{1}{2}\right)}{\Gamma(\frac{n-1}{2})} \cdot \left(\frac{1}{2}\right)^{-1/2}
$$
即
$$
\frac{\sqrt{n-1}}{\sigma} E(s) = \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma(\frac{n-1}{2})} \cdot \left(\frac{1}{2}\right)^{-1/2}
$$
因此，
$$
E(s) = \sqrt{\frac{2}{n-1}} \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)} \sigma .
$$
而$c_n = \left(\sqrt{\frac{2}{n-1}} \frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}\right)^{-1}$就是修偏系数。当$n$充分大时，这个修偏系数接近1。于是在实际使用时，我们通常忽略这个修偏系数，直接用样本标准差来估计总体标准差。
\end{example}
\begin{remark}
    \begin{enumerate}
        \item $s$是$\sigma$的有偏估计；
        \item $s$是$\sigma$的渐近无偏估计；
        \item $c_n s$是$\sigma$的无偏估计，但实际中我们不纠偏。
    \end{enumerate}
\end{remark}


值得注意的是，不是所有的参数都有无偏估计。在本课程中，如果一个参数的任何估计都不是无偏的，那么称这个参数\textbf{不可估}。以下介绍一个例子，供同学们课后阅读。
\begin{example}
设总体为二点分布$b(1,p),0<p<1$。$x_1,x_2,\cdots,x_n$是样本，令参数为$\theta = 1/p$。以下说明$\theta$是不可估的。    

首先，$T = x_1+x_2+\cdots+x_n$是充分统计量，则$T\sim b(n,p)$。若有一个$\hat{\theta} = \hat{\theta}(t)$是$\theta$的无偏估计，则有
$$
E(\hat{\theta}) = \sum_{i=1}^n \begin{pmatrix}
    n\\i
\end{pmatrix}
\hat{\theta}(i) p^{i}(1-p)^{n-i} = \frac{1}{p}
$$
也就是说，
$$
\sum_{i=1}^n \begin{pmatrix}
    n\\i
\end{pmatrix}
\hat{\theta}(i) p^{i+1}(1-p)^{n-i} -1 =0, \quad 0 < p < 1.
$$
这是$p$的$n+1$次方程，最多有$n+1$个实根，要使它对$(0,1)$中所有的$p$都成立是不可能的，故参数$\theta = 1/p$是不可估的。

其次，若有某个$h(x_1,x_2,\cdots,x_n)$是$\theta$的无偏估计，则令$\tilde{\theta} = E(h(x_1,x_2,\cdots,x_n)|T)$。由重期望公式可知，
$$
E(\tilde{\theta}) = E(E(h(x_1,x_2,\cdots,x_n)|T)) = E(h(x_1,x_2,\cdots,x_n) = \theta.
$$
这说明$\tilde{\theta}(T)$是$\theta$的无偏估计。因此这是不可能的。
\end{example}
\subsection{有效性}
\begin{problem}
    对于同一个参数$\theta$，无偏估计仍有很多，如何在无偏估计中进行选择？
\end{problem}
\begin{definition}
    设$\hat{\theta}_1,\hat{\theta}_2$是$\theta$的两个无偏估计。如果对任意的$\theta\in \Theta$有
    $$
    \text{Var}(\hat{\theta}_1) \leq \text{Var}(\hat{\theta}_2),
    $$
    且至少有一个$\theta \in \Theta$使得上述不等号严格成立，则称$\hat{\theta}_1$比$\hat{\theta}_2$更有效。
\end{definition}
\begin{remark}
    对无偏估计而言，方差越小的无偏估计越有效。
\end{remark}
\begin{example}\label{ex:chap20_UE_uniform_distribution}
设$x_1,x_2,\cdots,x_n$是来自均匀总体$U(0,\theta)$的样本，现有两个无偏估计。
\begin{enumerate}
    \item $\theta$的一个估计为$\hat{\theta}_1  = \frac{n+1}{n}x_{(n)}$。 因为$x_{(n)}/\theta\sim Be(n,1)$，所以$E(x_{(n)}) = \frac{n}{n+1}\theta$，而
    $$
    \text{Var}(x_{(n)}) = \frac{n}{(n+1)^2 (n+2)} \theta^2.
    $$
    于是，$\hat{\theta}_1$是$\theta$的无偏估计，其方差为
    $$
    \text{Var}(\hat{\theta}_1) = \frac{(n+1)^2}{n^2} \frac{n}{(n+1)^2 (n+2)} \theta^2 = \frac{\theta^2}{n(n+2)}.$$
    \item $\theta$的另一个估计为$\hat{\theta}_2 = 2\bar{x}$。可以计算
    $$
    E(\hat{\theta}_2) = 2 E(\bar{x}) = 2 \cdot \frac{\theta}{2} = \theta.
    $$
     而
     $$
     \text{Var}(\hat{\theta}_2)= 4 \text{Var}(\bar{x}) = \frac{4}{n} \cdot \frac{\theta^2}{12} = \frac{\theta^2}{3n}.
     $$
\end{enumerate}
根据比较可知，当$n>1$时，$\hat{\theta}_1$比$\hat{\theta}_2$更有效。
\end{example}
\begin{remark}
    我们发现：$\hat{\theta}_1$是$x_{(n)}$的函数，而$x_{(n)}$是$\theta$的充分统计量。
\end{remark}

\begin{problem}
对于任意$\theta \in \Theta$，我们能否找到一致最小均方误差估计$\hat{\theta}$，即
$$
\hat{\theta} = \arg\min_{\hat{\theta}} \text{MSE} (\hat{\theta})?
$$
\end{problem}
\begin{note}
    \vspace{5cm}
\end{note}
\begin{remark}
    在\textbf{无偏估计类}中，存在一致最小均方误差估计，此时一致最小均方误差估计为一致最小方差无偏估计（Uniformly Minimum Variance Unbiased Estimate, UMVUE）。对于UMVUE这里不再深入探讨，感兴趣的同学可以自行阅读《概率论与数理统计教程》6.4最小方差无偏估计这一节进行了解，属于选修内容。
\end{remark}
\subsection{充分性原则}

在例\ref{ex:chap20_UE_uniform_distribution}中，我们发现在两个无偏估计$\hat{\theta}_1 = \frac{n+1}{n}x_{(n)}$和$\hat{\theta}_2 = 2\bar{x}$中，更优的估计是$\hat{\theta}_1$，其是充分统计量$x_{(n)}$的函数。
\begin{problem}
    由充分统计量得到的估计更优，这是偶然吗？
\end{problem}

\begin{theorem}{Rao-Blackwell定理}
    设总体概率函数是$p(x;\theta)$, $x_1,x_2,\cdots,x_n$是其样本， $T = T(x_1,x_2,\cdots,x_n)$是$\theta$的充分统计量。则对$\theta$的任一无偏估计$\hat{\theta} = \hat{\theta}(x_1,x_2,\cdots,x_n)$，令$\tilde{\theta} = E(\hat{\theta}|T)$，有$\tilde{\theta}$也是$\theta$的无偏估计，且
    $$
    \text{Var}(\tilde{\theta}) \leq \text{Var}(\hat{\theta}).
    $$
\end{theorem}
\begin{proof}
第一，欲证明$\tilde{\theta}$是$\theta$的无偏估计，即
\begin{eqnarray*}
    E(\tilde{\theta}) = E( E(\hat{\theta}|T)) = E(\hat{\theta}) = \theta,
\end{eqnarray*}
其中，第二个等号成立是由于重期望公式。
第二，考察$\tilde{\theta}$的方差，即
\begin{eqnarray*}
\text{Var}(\hat{\theta}) &=&E(\hat{\theta}-E \hat{\theta})^{2} \\
&=&E(\hat{\theta}-\tilde{\theta}+\tilde{\theta}-E \tilde{\theta})^{2},E(\hat{\theta } )=E(\tilde{\theta }) \\
&=&E(\hat{\theta}-\tilde{\theta})^{2}+E(\tilde{\theta}-E(\tilde{\theta}))^{2} +2 E((\hat{\theta}-\tilde{\theta}) \cdot(\tilde{\theta}-E(\tilde{\theta}))) \\
&=&E(\hat{\theta}-\tilde{\theta})^{2}+\text{Var}(\tilde{\theta})
\end{eqnarray*}
其中交叉项为
\begin{eqnarray*}
    E((\hat{\theta}-\tilde{\theta}) \cdot(\tilde{\theta}-E(\tilde{\theta}))
&=& E(E((\hat{\theta}-\tilde{\theta}) \cdot(\tilde{\theta}-E(\tilde{\theta}) \mid T))\\
&=& E((\tilde{\theta}-E(\tilde{\theta})) E((\hat{\theta}-\tilde{\theta}) \mid T)) \\
&=& 0
\end{eqnarray*}
\end{proof}
\begin{remark}
    Rao-Blackwell定理表明，对于任何无偏估计，如果其不是充分统计量的函数，那么将其对充分统计量求条件期望可以得到一个新的无偏估计，而且该估计的方差比原来的估计方差要小。
\end{remark}
\begin{example}
总体分布为泊松分布$P(\lambda)$，其中参数$\lambda >0$。先有两个样本$x_1,x_2$。
$x_1,x_2$的联合分布列为
$$
p(x_1,x_2;\lambda) = \frac{\lambda^{x_1}}{x_1!}e^{-\lambda} \cdot \frac{\lambda^{x_2}}{x_2!}e^{-\lambda} = \lambda^{(x_1+x_2)} e^{-2\lambda}\frac{1}{x_1!x_2!}.
$$
令$T(x_1,x_2)=x_1+x_2$，$g(t,\lambda) = \lambda^{t} e^{-2\lambda}$和$h(x_1,x_2) = \frac{1}{x_1!x_2!}$。根据因子分解定理，有$T(x_1,x_2) = x_1+x_2$是充分的。而$T(x_1,x_2)\sim P(2\lambda)$。

因为$E(x_1) = \lambda$，所以$\hat{\lambda}= x_1$是$\lambda$的无偏估计。令$\tilde{\lambda} = E(\hat{\lambda}|T)$。则
\begin{eqnarray*}
    P(X_1 = x_1|X_1 + X_2 =n) &=& \frac{P(X_1=x_1,X_1+X_2=n)}{P(X_1+X_2=n)}\\
    &=& \frac{\frac{\lambda^{x_1}}{x_1!}e^{-\lambda} \frac{\lambda^{n-x_1}}{(n-x_1)!}e^{-\lambda} }{\frac{(2\lambda)^{n}}{n!}e^{-2\lambda} }\\
    &=& \frac{\frac{\lambda^{n}}{x_1!(n-x_1)!} }{\frac{(2\lambda)^{n}}{n!} }\\
    &=& \frac{n!}{x_1!(n-x_1)!} \left(1/2\right)^{x_1} \left(1/2\right)^{n-x_1} 
\end{eqnarray*}
且$$
E(X_1|X_1+X_2 = x_1+x_2) = \frac{x_1+x_2}{2}. 
$$
我们分别可以计算
$$
\text{Var}(\hat{\lambda}) = \lambda > 
\text{Var}(\tilde{\lambda}) = \frac{\lambda}{2}, \lambda> 0.
$$
所以，$\tilde{\lambda}$的方差更小。
\end{example}
\begin{remark}
    在考虑$\theta$的估计问题中，我们只需要基于充分统计量的函数来构造。这就是充分性原则，在所有统计推断的问题中都是成立的。
\end{remark}

以下例子由同学课后自己自行理解。
\begin{example}
设$x_1,x_2,\cdots,x_n$是来自$b(1,p)$的样本，则$\bar{x}$（或$T = n\bar{x}$）是$p$的充分统计量。为估计$\theta = p^2$，可令$$
\hat{\theta}_1 =\left\{
\begin{aligned}
& 1, & x_1 = 1, x_2 = 1,\\
& 0, & \text{其他}.
\end{aligned}
\right.
$$
由于
$$
E(\hat{\theta}_1) = P(x_1 = 1, x_2 = 1) = p\cdot p = \theta,
$$
所以，$\hat{\theta}_1 $是$\theta$的无偏估计。这个估计并不好，因为它只用了两个观测值。但我们可以用Rao-Blackwell定理来优化这个估计。具体过程如下：
\begin{eqnarray*}
    \hat{\theta} &=& E(\hat{\theta}_1 | T= t) = P(\hat{\theta}_1 = 1 | T = t) \\
    &=& \frac{P(x_1=1,x_2=1,T =t)}{P(T=t)} = \frac{P(x_1=1,x_2=1,\sum_{i=3}^nx_i =t-2)}{P(T=t)}\\
    &=& \frac{p \cdot p \cdot \begin{pmatrix}
        n-2\\t-2
    \end{pmatrix}p^{t-2} (1-p)^{n-t}}{\begin{pmatrix}
        n\\t
    \end{pmatrix}p^{t} (1-p)^{n-t}} = \frac{\begin{pmatrix}
        n-2\\t-2
    \end{pmatrix}}{\begin{pmatrix}
        n\\t
    \end{pmatrix}}\\
    &= & \frac{t(t-1)}{n(n-1)}.
\end{eqnarray*}
其中，$t=\sum_{i=1}^n x_i$。可以验证，$\hat{\theta}$是$\theta$的无偏估计，且$\text{Var}(\hat{\theta}) < \text{Var}(\hat{\theta}_1) $。
\end{example}

\section{大样本的评估方式1——相合性}

相合性是参数估计的必要条件。

\begin{definition}[相合估计]
设$\theta \in \theta $为未知参数，$\hat{\theta}_{n}=\hat{\theta}_{n}\left(x_{1}, x_{2}, \cdots, x_{n}\right)$是$\theta$的一个估计，$n$为样本容量。若对任何一个$\varepsilon>0$，有
$$\lim _{n \rightarrow \infty} P\left(\left|\hat{\theta}_{n}-\theta\right| \geqslant \varepsilon\right)=0 $$
则称$\hat{\theta}_{n}$为参数$\theta$的相合估计。
\end{definition}

\begin{remark}
相合性的本质是$\hat{\theta}_{n}$依概率收敛于$\bm\theta$。
\end{remark}

\begin{example}
设$x_1,x_2,\cdots$是来自正态总体$N(\mu,\sigma^2)$的样本序列。由中心极限定理（CLT）可知，
\begin{enumerate}
    \item $\bar{x}$是$\mu$的相合估计；
    \item $s_{n}^{2}$是$\sigma^{2}$的相合估计；
    \item $s^{2}$是$\sigma^{2}$的相合估计。
\end{enumerate}
\end{example}
\begin{theorem}
设$\hat{\theta}_n = \hat{\theta}_n (x_1,x_2,\cdots,x_n)$是$\theta$的一个估计，若
$$
\lim_{n\rightarrow \infty} E(\hat{\theta}_n) = \theta,\quad \lim_{n\rightarrow \infty} \text{Var}(\hat{\theta}_n) = 0.
$$
则$\hat{\theta}$是$\theta$的相合估计。
\end{theorem}
\begin{proof}
我们考虑事件
$$
\left\{ |\hat{\theta}_n - \theta| \geq  \varepsilon \right\}
$$
其中，
$$
\hat{\theta}_n - \theta = \left(\hat{\theta}_n - E(\hat{\theta}_n)  \right) +  \left(  E(\hat{\theta}_n)  - \theta\right).
$$
于是，
$$
|\hat{\theta}_n - \theta|  \leq \left|\hat{\theta}_n - E(\hat{\theta}_n)  \right| +  \left|  E(\hat{\theta}_n)  - \theta\right|.
$$
如果$\left|\hat{\theta}_n - E(\hat{\theta}_n)  \right|  < \varepsilon/2$且$\left|  E(\hat{\theta}_n)  - \theta\right|< \varepsilon/2$，那么$\left|\hat{\theta}_n - \theta\right| <\varepsilon$。考虑其逆否命题，如果
$
\left|\hat{\theta}_n - \theta\right| \geq \varepsilon,
$
那么
$$\left\{\left|\hat{\theta}_n - E(\hat{\theta}_n)  \right|  \geq \varepsilon/2 \right\} \cup \left\{\left|  E(\hat{\theta}_n)  - \theta\right|\geq  \varepsilon/2\right\}.
$$
于是，当$n\rightarrow \infty$时，
\begin{eqnarray*}
    P\left( |\hat{\theta}_n - \theta| \geq \varepsilon \right) &\leq& P\left( \left|\hat{\theta}_n - E(\hat{\theta}_n)  \right|  \geq \varepsilon/2 \right) + P\left( \left|  E(\hat{\theta}_n)  - \theta\right|\geq  \varepsilon/2 \right)\\
    &\leq & \frac{4}{\varepsilon^2} \text{Var}(\hat{\theta}_n) + 0 \rightarrow 0 
\end{eqnarray*}
其中，第二个不等式满足因为切比雪夫不等式，即对任意的$\varepsilon >0$，有
$$
P\left( |\hat{\theta}_n - E(\hat{\theta}_n)| \geq \frac{\varepsilon}{2} \right)\leq \frac{4}{\varepsilon^2} \text{Var}(\hat{\theta}_n).
$$
因此，$\hat{\theta}_n$是$\theta$的相合估计。
\end{proof}
\begin{example}
    设$x_1,x_2,\cdots,x_n$是来自于均匀总体$U(0,\theta)$的样本。接下来我们考虑$\theta$的估计$\hat{\theta}_1 = x_{(n)}$。因为$x_{(n)} \sim Be(n,1)$，所以，当$n\rightarrow \infty$时，
    \begin{eqnarray*}
        E(\hat{\theta}_1) &=& \frac{n}{n+1}\theta \rightarrow \theta \\
        \text{Var}(\hat{\theta}_1) &=& \frac{n}{(n+1)^2 (n+2)}\theta \rightarrow 0.
    \end{eqnarray*} 
    由此可证，$x_{(n)}$是$\theta$的相合估计。
\end{example}
\begin{theorem}
    若$\hat{\theta}_{n1},\hat{\theta}_{n2},\cdots,\hat{\theta}_{nk}$分别是$\theta_1,\theta_2,\cdots,\theta_k$的相合估计， $\eta = g(\theta_1,\theta_2,\cdots,\theta_k)$是$\theta_1,\theta_2,\cdots,\theta_k$的连续函数，则$\hat{\eta}_n =g(\hat{\theta}_{n1},\hat{\theta}_{n2},\cdots,\hat{\theta}_{nk})$是$\eta$的相合估计。
\end{theorem}
\begin{proof}
    由函数$g$的连续性，对任意给定的$\varepsilon > 0$，存在一个$\delta>0$，当$|\hat{\theta}_{nj}-\theta_j|< \delta,j=1,2,\cdots,k$时，有
    $$
    \left| g(\hat{\theta}_{n1},\hat{\theta}_{n2},\cdots,\hat{\theta}_{nk})-g(\theta_{1},\theta_2,\cdots,\theta_{k}) \right| < \varepsilon
    $$
    又由$\hat{\theta}_{n1},\hat{\theta}_{n2},\cdots,\hat{\theta}_{nk}$的相合性，对给定的$\delta>0$，对任意给定的$\nu>0$，存在正整数$N$，使得$n\geq N$时，
    $$
    P\left( \left| \hat{\theta}_{nj} - \theta_j \right| \geq \delta \right) < \nu /k, kj=1,2,\cdots,k.
    $$
    从而有
    \begin{eqnarray*}
        P\left( \cap_{j=1}^k  \left\{\left| \hat{\theta}_{nj} - \theta_j \right| <\delta\right\} \right) &=& 1 - P\left( \cup_{j=1}^k \left\{ \left| \hat{\theta}_{nj} - \theta_j \right| \geq \delta \right\}\right)\\
        &\geq & 1 - \sum_{j=1}^k P\left(   \left| \hat{\theta}_{nj} - \theta_j \right| \geq \delta \right)\\
        &> & 1- k \cdot \nu /k = 1-\nu.
    \end{eqnarray*}
    于是，
    $$
    \cap_{j=1}^k  \left\{\left| \hat{\theta}_{nj} - \theta_j \right| <\delta\right\} \subset \left\{ |\hat{\eta}_n - \eta|< \varepsilon \right\}
    $$
    所以，
    $$
    P(|\hat{\eta}_n - \eta|< \varepsilon ) > 1-\nu.
    $$
    由$\nu$的任意性，定理得证。
\end{proof}
\begin{remark}
矩估计和最大似然估计均具有相合性。
\end{remark}

\section{大样本的评估方式2——渐近正态性}

\begin{definition}
    若$\hat{\theta}_n$是$\theta$的相合估计。若存在趋于$0$的非负常数序列$\sigma_n(\theta)$，使得
    $$
    \frac{\hat{\theta}-n \theta}{\sigma_n(\theta)}
    $$
    按分布收敛于标准正态分布。称$\hat{\theta}_n$是渐近正态的，又称$\hat{\theta}_n$服从渐近正态分布$N(\theta,\sigma_n^2(\theta))$，记为
    $$
    \hat{\theta}_n \sim AN (\theta,\sigma_n^2(\theta)).
    $$
    其中称$\sigma_n^2(\theta)$为$\hat{\theta}_n$的渐近方差。
\end{definition}
在三种估计方法中，矩估计、最小二乘估计以及最大似然估计在现有理论中一般可以论证其的渐近正态性。其中，最大似然估计的渐近正态性的表现具有一定普遍性，有以下定理。

\begin{theorem}
    设总体$X$有概率函数$p(x;\theta),\theta\in \Theta$，$\Theta$为非退化区间，假定
    \begin{enumerate}
        \item 对任意$x$，偏导数$\frac{\partial \ln p}{\partial \theta}, \frac{\partial^2 \ln p}{\partial \theta^2}, \frac{\partial^3 \ln p}{\partial \theta^3}$对所有$\theta \in \Theta$都存在；
        \item 对任意$\theta \in \Theta$，有
        $$
        \left|\frac{\partial \ln p}{\partial \theta}  \right| < F_1(x),\quad \left|\frac{\partial^2 \ln p}{\partial \theta^2}  \right| < F_2(x), \quad 
        \left|\frac{\partial^3 \ln p}{\partial \theta^3}  \right| < F_3(x),
        $$
        其中函数$F_1(x),F_2(x),F_3(x)$满足
        $$
        \int_{-\infty}^{\infty} F_1(x) \text{d} x < \infty, \int_{-\infty}^{\infty} F_2(x) \text{d} x < \infty, \sup_{\theta \in \Theta} \int_{-\infty}^{\infty} F_3(x) p(x;\theta)\text{d} x < \infty.
         $$
        \item Fisher信息量为
        $$
        I(\theta) = \int_{-\infty}^{\infty} \left(\frac{\partial \ln p}{\partial \theta}\right)^2 p(x;\theta) \text{d}x
        $$
        对任意$\theta \in \Theta$，$0<I(\theta)<\infty$。
    \end{enumerate}
    若$x_1,x_2,\cdots,x_n$是来自该总体的样本，则存在未知参数$\theta$的最大似然估计$\hat{\theta}_n = \hat{\theta}_n(x_1,x_2,\cdots,x_n)$，且$\hat{\theta}_n$具有相合性和渐近正态性，即
    $$
    \hat{\theta}_n \sim AN \left(\theta,\frac{1}{nI(\theta)}\right).
    $$
\end{theorem}
\begin{remark}
    值得关注的是，最大似然估计的渐近方差为样本量为$n$的样本的Fisher信息量——这是参数$\theta$的无偏估计的方差的CR下界。如果一个参数的无偏估计的方差恰好达到了CR下界，那么这个估计被称为\textbf{有效估计}，认为其实所有无偏估计中方差最小的估计，也就是说，在无偏估计中最优的估计。虽然MLE不一定都是无偏的，但MLE通常是渐近无偏的，而且其渐近方差可达到无偏估计的方差的CR下界。

    因此，一般认为，在\textbf{参数模型的假定正确}时，最大似然估计通常是最优的估计。
\end{remark}
\begin{example}
    设总体分布为泊松分布$P(\lambda)$，可以证明${\lambda}$的矩估计和最大似然估计为
    $$
    \hat{\lambda}_n = \bar{x}_n = \frac{1}{n} \sum_{i=1}^n x_i.
    $$
    根据中心极限定理可知，
    $$
    \frac{\hat{\lambda}_n - \lambda}{\sqrt{\lambda /n}} \overset{L}{\longrightarrow} N(0,1).
    $$
    所以，$\hat{\lambda}_n$是渐近正态的，即
    $$
    \hat{\lambda}_n \sim AN(\lambda, \lambda/n).
    $$
    总体分布的分布列为
    $$
    p(x;\lambda) = \frac{\lambda^x}{x!} e^{-\lambda}
    $$
    其对数为
    $$
    \ln p(x;\lambda) = x \ln \lambda - \ln x! - \lambda.
    $$
    其导数为
    $$
    \frac{\partial \ln p}{\partial \lambda} = \frac{x}{\lambda} -1
    $$
    所以，Fisher信息量为
    \begin{eqnarray*}
        I(\lambda) &=& \int_{-\infty}^{\infty} \left(\frac{\partial \ln p}{\partial \lambda}\right)^2 p(x;\lambda) \text{d}x \\
        &=& \sum_{x=0}^{\infty} \left( \frac{x}{\lambda} -1\right)^2 p(x;\lambda) \\
        &=& \frac{E(X^2)}{\lambda^2} - \frac{2E(X)}{\lambda} +1 \\
        &=& \frac{\lambda + \lambda^2}{\lambda^2} - 1 \\
        &=& \frac{1}{\lambda}.
    \end{eqnarray*}
    所以，$\hat{\theta}_n$服从渐近正态分布，即
    $$
    \hat{\theta}_n \sim AN (\lambda, \lambda/n).
    $$
\end{example}





\section{习题}
\begin{enumerate}
    \item 设$\hat{\theta}$是参数$\theta$的无偏估计，且有$\text{Var}(\hat{\theta})>0$，试证$\left(\hat{\theta}\right)^2$不是$\theta^2$的无偏估计。

\item 设$x_1,x_2,\cdots,x_n$是来自于下列总体的简单样本
$$
p(x;\theta) = \left\{
\begin{aligned}
    & 1, &\theta - \frac{1}{2} \leq x\leq \theta + \frac{1}{2},\\
& 0, &\text{其他},
\end{aligned}\right.
$$
证明样本均值$\bar{x}$及$\frac{1}{2}(x_{(1)}+x_{(n)})$都是$\theta$的无偏估计，问哪一个更为有效？（提示：从充分性原则来具体分析一下这个结论。）

\item 设从均值为$\mu$，方差为$\sigma^2>0$的总体中分布抽取容量为$n_1$和$n_2$的两独立样本，$\bar{x}_1$和$\bar{x}_2$是这两个样本的均值。试证，对于任意常数$a,b (a+b=1)$，$Y = a\bar{x}_1 + b\bar{x}_2$都是$\mu$的无偏估计，并确定常数$a,b$使得$\text{Var}(Y)$达到最小。

\item 设总体密度函数为
$$p(x;\theta) = \theta x^{\theta - 1}, 0<x<1,\theta > 0.$$
$x_1,x_2,\cdots,x_n$是样本。求$g(\theta)  =  1/\theta$的最大似然估计。

\item 设总体$X\sim Exp(1/\theta)$, $x_1,x_2,\cdots,x_n$是样本。
\begin{enumerate}
    \item  验证$\theta$的矩估计和最大似然估计都是$\bar{x}$；
\item 验证$\bar{x}$也是$\theta$的相合估计和无偏估计；
\item 试证明在均方误差准则下存在优于$\bar{x}$的估计。（提示：考虑$\hat{\theta}_n = a \bar{x}$，找均方误差最小者）
\end{enumerate}


\item 设$x_1,x_2,\cdots,x_n$是来自密度函数为
$$ 
p(x;\theta) = e^{-(x-\theta)},
x>\theta
$$
的总体的样本。
\begin{enumerate}
    \item 求$\theta$的最大似然估计$\hat{\theta}_1$, 它是否是相合估计？是否是无偏估计？
    \item 求$\theta$的矩估计$\hat{\theta}_2$，它是否是相合估计？它是否是无偏估计？
\end{enumerate}
\end{enumerate}